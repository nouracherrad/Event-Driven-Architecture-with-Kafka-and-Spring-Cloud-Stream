

```markdown
# üß© Event-Driven Architecture with Kafka and Spring Cloud Stream

## üìñ Description du projet
Ce projet illustre la mise en place d‚Äôune **architecture pilot√©e par les √©v√©nements (Event-Driven Architecture)** en utilisant **Apache Kafka** et **Spring Cloud Stream**.  
Il a √©t√© r√©alis√© dans le cadre de l‚Äô**Activit√© Pratique n¬∞1** du cours de Mohamed Youssfi, visant √† comprendre la communication asynchrone entre microservices, le traitement temps r√©el et l‚Äôanalyse de flux de donn√©es.

## ‚öôÔ∏è Objectifs p√©dagogiques
- Comprendre le fonctionnement de **Kafka** et de **Zookeeper**.
- Cr√©er un **Producer**, un **Consumer** et un **Supplier** Kafka avec Spring Cloud Stream.
- Mettre en ≈ìuvre un **pipeline de traitement de flux** avec Kafka Streams.
- Visualiser les r√©sultats de l‚Äôanalyse de flux en temps r√©el via une interface Web.

## üß± Architecture du projet
```

+-------------------------+
|  PageEventSupplier      |  --> Produit des √©v√©nements (T3)
+-------------------------+
‚Üì
+-------------------------+
|  KStreamFunction        |  --> Filtre, groupe et compte les √©v√©nements (T4)
+-------------------------+
‚Üì
+-------------------------+
|  PageEventConsumer      |  --> Consomme et affiche les √©v√©nements filtr√©s
+-------------------------+
‚Üì
+-------------------------+
|  PageEventController    |  --> Expose les endpoints REST et analytics
+-------------------------+
‚Üì
+-------------------------+
|  HTML + Smoothie.js     |  --> Visualisation temps r√©el
+-------------------------+

````

## üß© Technologies utilis√©es
- **Java 17**
- **Spring Boot 3.x**
- **Spring Cloud Stream**
- **Apache Kafka**
- **Kafka Streams API**
- **Docker / Docker Compose**
- **HTML5 / JavaScript (Smoothie.js)**

## üê≥ D√©marrage avec Docker
1. Cr√©er et lancer les conteneurs Kafka et Zookeeper :
```bash
docker-compose up -d
````

2. V√©rifier que les conteneurs sont en cours d‚Äôex√©cution :

```bash
docker ps
```

Tu devrais voir :

* `bdcc-zookeeper`
* `bdcc-kafla-broker`

## üöÄ Ex√©cution du projet Spring Boot

1. Ouvre le projet dans **IntelliJ IDEA**.
2. Lance l‚Äôapplication Spring Boot (`KafkaSpringCloudStreamApplication`).
3. Acc√®de aux endpoints :

   * **G√©n√©rer un √©v√©nement :**

     ```
     http://localhost:8080/publish?name=Page1&topic=T3
     ```
   * **Visualiser les analyses en temps r√©el :**
     Ouvre le fichier **analytics.html** dans ton navigateur pour voir le graphe dynamique.

## üì° Configuration Kafka Streams

Dans `application.properties` :

```properties
spring.application.name=kafka-spring-cloud-stream
server.port=8080

spring.cloud.stream.bindings.pageEventSupplier-out-0.destination=T3
spring.cloud.stream.bindings.kStreamFunction-in-0.destination=T3
spring.cloud.stream.bindings.kStreamFunction-out-0.destination=T4
spring.cloud.stream.bindings.pageEventConsumer-in-0.destination=T2
spring.cloud.function.definition=pageEventConsumer;pageEventSupplier;kStreamFunction
spring.cloud.stream.bindings.pageEventSupplier-out-0.producer.poller.fixed-delay=200
spring.cloud.stream.kafka.streams.binder.configuration.commit.interval.ms=1
```

## üìù Explications du projet

### 1. Mise en place de l‚Äôenvironnement Kafka

* Installation et configuration de **Zookeeper** et **Kafka Broker**.
* Deux options utilis√©es :

  * **Localement**, via les binaires Kafka pour tester les commandes `kafka-console-producer` et `kafka-console-consumer`.
  * **Avec Docker**, en utilisant `docker-compose.yml`.

### 2. Cr√©ation des services Kafka avec Spring Cloud Stream

* **Producer (`pageEventSupplier`)** : g√©n√®re des √©v√©nements `PageEvent` et les publie sur le topic `T3`.
* **Consumer (`pageEventConsumer`)** : consomme les √©v√©nements depuis le topic `T2` et les affiche.
* **KStream Function (`kStreamFunction`)** : filtre, groupe et compte les √©v√©nements par page sur des fen√™tres de 5 secondes, puis les envoie sur le topic `T4`.

### 3. Contr√¥leur REST (`PageEventController`)

* Endpoint `/publish` : permet de pousser un √©v√©nement sur un topic Kafka depuis le navigateur ou Postman.
* Endpoint `/analytics` : fournit les donn√©es d‚Äôanalyse en **temps r√©el** via **SSE** (Server-Sent Events).
* Utilisation de **InteractiveQueryService** pour interroger le **state store** de Kafka Streams et r√©cup√©rer les comptes par page.

### 4. Visualisation Web

* Fichier **`analytics.html`** utilisant **Smoothie.js** pour tracer un **graphe dynamique** des √©v√©nements en temps r√©el.

### 5. R√©sultat final

Apr√®s avoir d√©marr√© les conteneurs Docker et l‚Äôapplication Spring Boot :

* Les √©v√©nements sont publi√©s automatiquement par `pageEventSupplier`.
* Les √©v√©nements filtr√©s et agr√©g√©s apparaissent dans la console via `pageEventConsumer`.
* Le graphe dynamique affiche en temps r√©el le nombre d‚Äô√©v√©nements par page.

**üì∏ Screen du r√©sultat final :**
*(√Ä ajouter ici apr√®s capture de l‚Äô√©cran)*

```

---

Si tu veux, je peux te faire une **version am√©lior√©e avec badges GitHub, Maven, Docker et Java**, ce qui rendra ton README encore plus attractif pour GitHub.  

Veux‚Äëtu que je fasse √ßa‚ÄØ?
```
