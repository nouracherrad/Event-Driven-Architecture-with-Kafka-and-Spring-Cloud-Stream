
#  Event-Driven Architecture with Kafka and Spring Cloud Stream

## Description du projet
Ce projet illustre la mise en place d‚Äôune **architecture pilot√©e par les √©v√©nements (Event-Driven Architecture)** en utilisant **Apache Kafka** et **Spring Cloud Stream**, visant √† comprendre la communication asynchrone entre microservices, le traitement temps r√©el et l‚Äôanalyse de flux de donn√©es.

##  Objectifs p√©dagogiques
- Comprendre le fonctionnement de **Kafka** et de **Zookeeper**.
- Cr√©er un **Producer**, un **Consumer** et un **Supplier** Kafka avec Spring Cloud Stream.
- Mettre en ≈ìuvre un **pipeline de traitement de flux** avec Kafka Streams.
- Visualiser les r√©sultats de l‚Äôanalyse de flux en temps r√©el via une interface Web.

##  Architecture du projet
```

+-------------------------+
|  PageEventSupplier      |  --> Produit des √©v√©nements (T3)
+-------------------------+
‚Üì
+-------------------------+
|  KStreamFunction        |  --> Filtre, groupe et compte les √©v√©nements (T4)
+-------------------------+
‚Üì
+-------------------------+
|  PageEventConsumer      |  --> Consomme et affiche les √©v√©nements filtr√©s
+-------------------------+
‚Üì
+-------------------------+
|  PageEventController    |  --> Expose les endpoints REST et analytics
+-------------------------+
‚Üì
+-------------------------+
|  HTML   |  --> Visualisation temps r√©el
+-------------------------+

````



## üê≥ D√©marrage avec Docker
1. Cr√©er et lancer les conteneurs Kafka et Zookeeper :
```bash
docker-compose up -d
````

2. V√©rifier que les conteneurs sont en cours d‚Äôex√©cution :

```bash
docker ps
```

Tu devrais voir :

* `bdcc-zookeeper`
* `bdcc-kafla-broker`

##  Ex√©cution du projet Spring Boot

1. Ouvre le projet dans **IntelliJ IDEA**.
2. Lance l‚Äôapplication Spring Boot (`KafkaSpringCloudStreamApplication`).
3. Acc√®de aux endpoints :

   * **G√©n√©rer un √©v√©nement :**

     ```
     http://localhost:8080/publish?name=Page1&topic=T3
     ```

     <img width="604" height="306" alt="image" src="https://github.com/user-attachments/assets/318fff50-980a-4642-8879-2832b536b865" />

   * **Visualiser les analyses en temps r√©el :**
     Ouvre le fichier **analytics.html** dans ton navigateur pour voir le graphe dynamique.
<img width="1319" height="546" alt="image" src="https://github.com/user-attachments/assets/7f592019-3dbd-4d32-b3c3-1a44a7489c1a" />

##  Configuration Kafka Streams

Dans `application.properties` :

```properties
spring.application.name=kafka-spring-cloud-stream
server.port=8080

spring.cloud.stream.bindings.pageEventSupplier-out-0.destination=T3
spring.cloud.stream.bindings.kStreamFunction-in-0.destination=T3
spring.cloud.stream.bindings.kStreamFunction-out-0.destination=T4
spring.cloud.stream.bindings.pageEventConsumer-in-0.destination=T2
spring.cloud.function.definition=pageEventConsumer;pageEventSupplier;kStreamFunction
spring.cloud.stream.bindings.pageEventSupplier-out-0.producer.poller.fixed-delay=200
spring.cloud.stream.kafka.streams.binder.configuration.commit.interval.ms=1
```

##  Explications du projet

### 1. Mise en place de l‚Äôenvironnement Kafka

* Installation et configuration de **Zookeeper** et **Kafka Broker**.
* Deux options utilis√©es :

  * **Localement**, via les binaires Kafka pour tester les commandes `kafka-console-producer` et `kafka-console-consumer`.
 
 ![Uploading image.png‚Ä¶]()


  * **Avec Docker**, en utilisant `docker-compose.yml`.
  * et voici le container
<img width="1293" height="585" alt="image" src="https://github.com/user-attachments/assets/74aa7eb9-06f5-49ac-aaca-a3fc060bd92d" />



### 2. Cr√©ation des services Kafka avec Spring Cloud Stream

* **Producer (`pageEventSupplier`)** : g√©n√®re des √©v√©nements `PageEvent` et les publie sur le topic `T3`.
* **Consumer (`pageEventConsumer`)** : consomme les √©v√©nements depuis le topic `T2` et les affiche.
* **KStream Function (`kStreamFunction`)** : filtre, groupe et compte les √©v√©nements par page sur des fen√™tres de 5 secondes, puis les envoie sur le topic `T4`.

### 3. Contr√¥leur REST (`PageEventController`)

* Endpoint `/publish` : permet de pousser un √©v√©nement sur un topic Kafka depuis le navigateur ou Postman.
* Endpoint `/analytics` : fournit les donn√©es d‚Äôanalyse en **temps r√©el** via **SSE** (Server-Sent Events).
* Utilisation de **InteractiveQueryService** pour interroger le **state store** de Kafka Streams et r√©cup√©rer les comptes par page.

### 4. Visualisation Web

* Fichier **`analytics.html`** utilisant **Smoothie.js** pour tracer un **graphe dynamique** des √©v√©nements en temps r√©el.

### 5. R√©sultat final

Apr√®s avoir d√©marr√© les conteneurs Docker et l‚Äôapplication Spring Boot :

* Les √©v√©nements sont publi√©s automatiquement par `pageEventSupplier`.
* Les √©v√©nements filtr√©s et agr√©g√©s apparaissent dans la console via `pageEventConsumer`.
* Le graphe dynamique affiche en temps r√©el le nombre d‚Äô√©v√©nements par page.

**Screen du r√©sultat final :**
<img width="1039" height="643" alt="image" src="https://github.com/user-attachments/assets/6c6ba84b-80a0-4f79-b274-24c4d91cfeea" />

```





